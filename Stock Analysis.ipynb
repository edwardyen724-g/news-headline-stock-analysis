{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a stock price prediction project that closely follows this website:https://medium.com/@Currie32/predicting-the-stock-market-with-the-news-and-deep-learning-7fc8f5f639bc\n",
    "full project github: https://github.com/Currie32/Predicting-the-Dow-Jones-with-Headlines\n",
    "model description: https://www.aclweb.org/anthology/C16-1229.pdf\n",
    "Primarily uses Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj = pd.read_csv(\"DowJones.csv\")\n",
    "news = pd.read_csv(\"RedditNews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>17924.240234</td>\n",
       "      <td>18002.380859</td>\n",
       "      <td>17916.910156</td>\n",
       "      <td>17949.369141</td>\n",
       "      <td>82160000</td>\n",
       "      <td>17949.369141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>17712.759766</td>\n",
       "      <td>17930.609375</td>\n",
       "      <td>17711.800781</td>\n",
       "      <td>17929.990234</td>\n",
       "      <td>133030000</td>\n",
       "      <td>17929.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-29</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17704.509766</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17694.679688</td>\n",
       "      <td>106380000</td>\n",
       "      <td>17694.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>112190000</td>\n",
       "      <td>17409.720703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17063.080078</td>\n",
       "      <td>17140.240234</td>\n",
       "      <td>138740000</td>\n",
       "      <td>17140.240234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date          Open          High           Low         Close  \\\n",
       "0  2016-07-01  17924.240234  18002.380859  17916.910156  17949.369141   \n",
       "1  2016-06-30  17712.759766  17930.609375  17711.800781  17929.990234   \n",
       "2  2016-06-29  17456.019531  17704.509766  17456.019531  17694.679688   \n",
       "3  2016-06-28  17190.509766  17409.720703  17190.509766  17409.720703   \n",
       "4  2016-06-27  17355.210938  17355.210938  17063.080078  17140.240234   \n",
       "\n",
       "      Volume     Adj Close  \n",
       "0   82160000  17949.369141  \n",
       "1  133030000  17929.990234  \n",
       "2  106380000  17694.679688  \n",
       "3  112190000  17409.720703  \n",
       "4  138740000  17140.240234  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Volume       0\n",
       "Adj Close    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date    0\n",
       "News    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               News\n",
       "0  2016-07-01  A 117-year-old woman in Mexico City finally re...\n",
       "1  2016-07-01   IMF chief backs Athens as permanent Olympic host\n",
       "2  2016-07-01  The president of France says if Brexit won, so...\n",
       "3  2016-07-01  British Man Who Must Give Police 24 Hours' Not...\n",
       "4  2016-07-01  100+ Nobel laureates urge Greenpeace to stop o..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1989, 7)\n",
      "(73608, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dj.shape)\n",
    "print(news.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news[news.Date.isin(dj.Date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989\n",
      "1989\n"
     ]
    }
   ],
   "source": [
    "print(len(set(dj.Date)))\n",
    "print(len(set(news.Date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here the dj becomes the difference between the opening price of the day and the next date\n",
    "# set_index first so that when .diff calculate the difference in price it won't effect the date\n",
    "dj = dj.set_index('Date').diff(periods=1)\n",
    "dj['Date'] = dj.index\n",
    "dj = dj.reset_index(drop=True)\n",
    "\n",
    "#drop unnecessary columns\n",
    "dj = dj.drop(['High','Low','Close','Volume','Adj Close'],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-211.480468</td>\n",
       "      <td>2016-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-256.740235</td>\n",
       "      <td>2016-06-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-265.509765</td>\n",
       "      <td>2016-06-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>164.701172</td>\n",
       "      <td>2016-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1984</td>\n",
       "      <td>-79.139649</td>\n",
       "      <td>2008-08-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1985</td>\n",
       "      <td>100.739258</td>\n",
       "      <td>2008-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1986</td>\n",
       "      <td>148.890625</td>\n",
       "      <td>2008-08-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1987</td>\n",
       "      <td>-52.030273</td>\n",
       "      <td>2008-08-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1988</td>\n",
       "      <td>-297.580078</td>\n",
       "      <td>2008-08-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1989 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open        Date\n",
       "0            NaN  2016-07-01\n",
       "1    -211.480468  2016-06-30\n",
       "2    -256.740235  2016-06-29\n",
       "3    -265.509765  2016-06-28\n",
       "4     164.701172  2016-06-27\n",
       "...          ...         ...\n",
       "1984  -79.139649  2008-08-14\n",
       "1985  100.739258  2008-08-13\n",
       "1986  148.890625  2008-08-12\n",
       "1987  -52.030273  2008-08-11\n",
       "1988 -297.580078  2008-08-08\n",
       "\n",
       "[1989 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj = dj[dj.Open.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = []\n",
    "headlines = []\n",
    "\n",
    "#row now is an index series, where the series is of the form [Open, Date] pair, therefore call row[1] to only get the series, which is the value of each row\n",
    "for row in dj.iterrows():\n",
    "    daily_headlines = []\n",
    "    date = row[1][\"Date\"]\n",
    "    price.append(row[1]['Open'])\n",
    "    # collect news from the same date and put them into daily_headlines\n",
    "    for row_ in news[news.Date==date].iterrows():\n",
    "        daily_headlines.append(row_[1]['News'])\n",
    "    headlines.append(daily_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988\n",
      "1988\n"
     ]
    }
   ],
   "source": [
    "print(len(price))\n",
    "print(len(headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'0,0', '00', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'\\$', ' $ ', text)\n",
    "    text = re.sub(r'u s ', ' united states ', text)\n",
    "    text = re.sub(r'u n ', ' united nations ', text)\n",
    "    text = re.sub(r'u k ', ' united kingdom ', text)\n",
    "    text = re.sub(r'j k ', ' jk ', text)\n",
    "    text = re.sub(r' s ', ' ', text)\n",
    "    text = re.sub(r' yr ', ' year ', text)\n",
    "    text = re.sub(r' l g b t ', ' lgbt ', text)\n",
    "    text = re.sub(r'0km ', '0 km ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the headlines\n",
    "clean_headlines = []\n",
    "\n",
    "for daily_headlines in headlines:\n",
    "    clean_daily_headlines = []\n",
    "    for headline in daily_headlines:\n",
    "        clean_daily_headlines.append(clean_text(headline))\n",
    "    clean_headlines.append(clean_daily_headlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jamaica proposes marijuana dispensers tourists airports following legalisation kiosks desks would give people license purchase 2 ounces drug use stay',\n",
       " 'stephen hawking says pollution stupidity still biggest threats mankind certainly become less greedy less stupid treatment environment past decade',\n",
       " 'boris johnson says run tory party leadership',\n",
       " 'six gay men ivory coast abused forced flee homes pictured signing condolence book victims recent attack gay nightclub florida',\n",
       " 'switzerland denies citizenship muslim immigrant girls refused swim boys report',\n",
       " 'palestinian terrorist stabs israeli teen girl death bedroom',\n",
       " 'puerto rico default $ 1 billion debt friday',\n",
       " 'republic ireland fans awarded medal sportsmanship paris mayor',\n",
       " 'afghan suicide bomber kills 40 bbc news',\n",
       " 'us airstrikes kill least 250 isis fighters convoy outside fallujah official says',\n",
       " 'turkish cop took istanbul gunman hailed hero',\n",
       " 'cannabis compounds could treat alzheimer removing plaque forming proteins brain cells research suggests',\n",
       " 'japan top court approved blanket surveillance country muslims made us terrorist suspects never anything wrong says japanese muslim mohammed fujita',\n",
       " 'cia gave romania millions host secret prisons',\n",
       " 'groups urge united nations suspend saudi arabia rights council',\n",
       " 'googles free wifi indian railway stations better countrys paid services',\n",
       " 'mounting evidence suggests hobbits wiped modern humans ancestors 50000 years ago',\n",
       " 'men carried tuesday terror attack istanbul ataturk airport russia uzbekistan kyrgyzstan turkish offical said',\n",
       " 'calls suspend saudi arabia un human rights council military aggresion yemen',\n",
       " '100 nobel laureates call greenpeace anti gmo obstruction developing world',\n",
       " 'british pedophile sentenced 85 years us trafficking child abuse images domminich shaw kingpin sexual violence children sent dozens images online discussed plans assault kill child probation',\n",
       " 'us permitted 1 200 offshore fracks gulf mexico 2010 2014 allowed 72 billion gallons chemical discharge 2014',\n",
       " 'swimming ridicule french beach police carry guns swimming trunks police lifeguards frances busiest beaches carry guns bullet proof vests first time summer amid fears terrorists could target holidaymakers',\n",
       " 'uefa says minutes silence istanbul victims euro 2016 turkey already eliminated',\n",
       " 'law enforcement sources gun used paris terrorist attacks came phoenix']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_headlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 35190\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "\n",
    "for date in clean_headlines:\n",
    "    for headline in date:\n",
    "        for word in headline.split():\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 1\n",
    "            else:\n",
    "                word_counts[word] += 1\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embeddings: 2196016\n"
     ]
    }
   ],
   "source": [
    "# load GLoVe's embeddings\n",
    "embeddings_index = {}\n",
    "with open('C:/Users/edwar/Projects/Stock Prediction/glove.840B.300d.txt', encoding ='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "    f.close()        \n",
    "print(\"word embeddings:\", len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from GloVe: 47\n",
      "Percent of words that are missing from vocabulary: 0.13%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from GloVe, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 10\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from GloVe:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Unique Words: 35190\n",
      "Number of Words we will use: 31265\n",
      "Percent of Words we will use: 88.85%\n"
     ]
    }
   ],
   "source": [
    "# Use only the words that appear more than a set threashold or that appears in GLoVe.\n",
    "# creates two dictionary that gives indexing value to each word that is used\n",
    " \n",
    "value = 0\n",
    "vocab_to_int = {}\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value \n",
    "        value += 1\n",
    "        \n",
    "# Special tokens that will be added to our vocab <UNK> = unknown, <PAD> = \n",
    "codes = [\"<UNK>\",\"<PAD>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total Number of Unique Words:\", len(word_counts))\n",
    "print(\"Number of Words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of Words we will use: {}%\".format(usage_ratio))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31263\n",
      "31264\n"
     ]
    }
   ],
   "source": [
    "for code in codes:\n",
    "    print(vocab_to_int[code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31265\n"
     ]
    }
   ],
   "source": [
    "#Create New embeddings for words in word_counts that are not in embedding_index \n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "#total number of words to use in training\n",
    "\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "#create a embedding matrix, each row is for a word, with total of 300 columns for the dimension of the GloVe vectors\n",
    "\n",
    "word_embedding_matrix = np.zeros((nb_words,embedding_dim))\n",
    "\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        #create a new embedding vectors with random values for words not in GLoVe embedding\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        # we also add this newly generated random embedding to the embedding_index along with the rest of the GloVe embedding\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "        \n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the words in the embedding \"including the pretrained ones\" will be updated as the model train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 615989\n",
      "Total number of UNKs in headlines: 5262\n",
      "Percent of words that are UNK: 0.8542%\n"
     ]
    }
   ],
   "source": [
    "# now to create a replicated headlines where all the words are transformed into their regarding index in vocab_to_int \n",
    "# int_headlines: all the headlines\n",
    "# int_daily_headlines: all the headlines in a day\n",
    "# int_headline: single headline in a day \n",
    "# int_headlines[int_daily_headlines[int_headline]]\n",
    "\n",
    "# for words in the headlines that are not in vocab_to_int \n",
    "# (meaning the word didn't hit the threshold and isn't in the pretrained embedding index)\n",
    "# we replace it with the <UNK> token for \"unknown\"\n",
    "word_count = 0\n",
    "unk_count = 0 \n",
    "\n",
    "int_headlines = []\n",
    "\n",
    "for date in clean_headlines:\n",
    "    int_daily_headlines = []\n",
    "    for headline in date:\n",
    "        int_headline = []\n",
    "        for word in headline.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                int_headline.append(vocab_to_int[word])\n",
    "            else:\n",
    "                int_headline.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        int_daily_headlines.append(int_headline)\n",
    "    int_headlines.append(int_daily_headlines)\n",
    "\n",
    "unk_percent = round((unk_count/word_count)*100,4)\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length of individual headline\n",
    "lengths = []\n",
    "for date in int_headlines:\n",
    "    for headline in date:\n",
    "        lengths.append(len(headline))\n",
    "\n",
    "lengths = pd.DataFrame(lengths, columns=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>49693.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>12.395891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>6.790246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "count  49693.000000\n",
       "mean      12.395891\n",
       "std        6.790246\n",
       "min        1.000000\n",
       "25%        7.000000\n",
       "50%       10.000000\n",
       "75%       16.000000\n",
       "max       41.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the length of total words per day to 200 words\n",
    "# and headlines to 16 words to reduce training time\n",
    "\n",
    "max_headline_length = 16\n",
    "max_daily_length = 200\n",
    "# pad_headlines only divides up the news up to date, not individual news (2D)\n",
    "# for news length that does not reach the max_daily_length, we pad it with the <PAD> token \n",
    "\n",
    "pad_headlines = []\n",
    "\n",
    "for date in int_headlines:\n",
    "    pad_daily_headlines = []\n",
    "    for headline in date:\n",
    "        #add the headline to pad daily headline if headline is less than max length\n",
    "        if len(headline) <= max_headline_length:\n",
    "            for word in headline:\n",
    "                #needs to append word by word so pad_daily_headline is 1D\n",
    "                pad_daily_headlines.append(word)\n",
    "        else:\n",
    "            headline = headline[:max_headline_length]\n",
    "            for word in headline:\n",
    "                pad_daily_headlines.append(word)\n",
    "    if len(pad_daily_headlines) < max_daily_length:\n",
    "        for i in range(max_daily_length - len(pad_daily_headlines)):\n",
    "            pad = vocab_to_int[\"<PAD>\"]\n",
    "            pad_daily_headlines.append(pad)\n",
    "    else:\n",
    "        pad_daily_headlines = pad_daily_headlines[:max_daily_length]\n",
    "    pad_headlines.append(pad_daily_headlines)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize (between [0,1]) the opening prices(target values, response variable)\n",
    "max_price = max(price)\n",
    "min_price = min(price)\n",
    "mean_price = np.mean(price)\n",
    "\n",
    "def normalize(price):\n",
    "    return ((price-min_price)/(max_price-min_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_price = []\n",
    "for p in price:\n",
    "    norm_price.append(normalize(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.5448422454901358\n"
     ]
    }
   ],
   "source": [
    "# checking normalization\n",
    "print(min(norm_price))\n",
    "print(max(norm_price))\n",
    "print(np.mean(norm_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train test split\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(pad_headlines, norm_price, test_size=0.15, random_state=2)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1689\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "# Check the lengths\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
