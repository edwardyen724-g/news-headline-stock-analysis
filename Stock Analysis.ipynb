{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a stock price prediction project that basically copies from this website:https://medium.com/@Currie32/predicting-the-stock-market-with-the-news-and-deep-learning-7fc8f5f639bc\n",
    "full project github: https://github.com/Currie32/Predicting-the-Dow-Jones-with-Headlines\n",
    "model description: https://www.aclweb.org/anthology/C16-1229.pdf\n",
    "Primarily uses Keras\n",
    "This project is mainly for me to familiarize myself in using Keras libraries and packages. It is also a way for me to learning about word embedding and some basic ideas about NLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import median_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras.layers import Dropout, Activation, Embedding, Convolution1D\n",
    "from keras.layers import MaxPooling1D, Input, Dense, BatchNormalization, Flatten, Reshape, Concatenate\n",
    "\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras import regularizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj = pd.read_csv(\"dataset/DowJones.csv\")\n",
    "news = pd.read_csv(\"dataset/RedditNews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>17924.240234</td>\n",
       "      <td>18002.380859</td>\n",
       "      <td>17916.910156</td>\n",
       "      <td>17949.369141</td>\n",
       "      <td>82160000</td>\n",
       "      <td>17949.369141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>17712.759766</td>\n",
       "      <td>17930.609375</td>\n",
       "      <td>17711.800781</td>\n",
       "      <td>17929.990234</td>\n",
       "      <td>133030000</td>\n",
       "      <td>17929.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-06-29</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17704.509766</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17694.679688</td>\n",
       "      <td>106380000</td>\n",
       "      <td>17694.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>112190000</td>\n",
       "      <td>17409.720703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17063.080078</td>\n",
       "      <td>17140.240234</td>\n",
       "      <td>138740000</td>\n",
       "      <td>17140.240234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date          Open          High           Low         Close  \\\n",
       "0  2016-07-01  17924.240234  18002.380859  17916.910156  17949.369141   \n",
       "1  2016-06-30  17712.759766  17930.609375  17711.800781  17929.990234   \n",
       "2  2016-06-29  17456.019531  17704.509766  17456.019531  17694.679688   \n",
       "3  2016-06-28  17190.509766  17409.720703  17190.509766  17409.720703   \n",
       "4  2016-06-27  17355.210938  17355.210938  17063.080078  17140.240234   \n",
       "\n",
       "      Volume     Adj Close  \n",
       "0   82160000  17949.369141  \n",
       "1  133030000  17929.990234  \n",
       "2  106380000  17694.679688  \n",
       "3  112190000  17409.720703  \n",
       "4  138740000  17140.240234  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Volume       0\n",
       "Adj Close    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date    0\n",
       "News    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               News\n",
       "0  2016-07-01  A 117-year-old woman in Mexico City finally re...\n",
       "1  2016-07-01   IMF chief backs Athens as permanent Olympic host\n",
       "2  2016-07-01  The president of France says if Brexit won, so...\n",
       "3  2016-07-01  British Man Who Must Give Police 24 Hours' Not...\n",
       "4  2016-07-01  100+ Nobel laureates urge Greenpeace to stop o..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1989, 7)\n",
      "(73608, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dj.shape)\n",
    "print(news.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news[news.Date.isin(dj.Date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989\n",
      "1989\n"
     ]
    }
   ],
   "source": [
    "print(len(set(dj.Date)))\n",
    "print(len(set(news.Date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here the dj becomes the difference between the opening price of the day and the next date\n",
    "# set_index first so that when .diff calculate the difference in price it won't effect the date\n",
    "dj = dj.set_index('Date').diff(periods=1)\n",
    "dj['Date'] = dj.index\n",
    "dj = dj.reset_index(drop=True)\n",
    "\n",
    "#drop unnecessary columns\n",
    "dj = dj.drop(['High','Low','Close','Volume','Adj Close'],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-211.480468</td>\n",
       "      <td>2016-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-256.740235</td>\n",
       "      <td>2016-06-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-265.509765</td>\n",
       "      <td>2016-06-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164.701172</td>\n",
       "      <td>2016-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>-79.139649</td>\n",
       "      <td>2008-08-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>100.739258</td>\n",
       "      <td>2008-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>148.890625</td>\n",
       "      <td>2008-08-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>-52.030273</td>\n",
       "      <td>2008-08-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>-297.580078</td>\n",
       "      <td>2008-08-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1989 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open        Date\n",
       "0            NaN  2016-07-01\n",
       "1    -211.480468  2016-06-30\n",
       "2    -256.740235  2016-06-29\n",
       "3    -265.509765  2016-06-28\n",
       "4     164.701172  2016-06-27\n",
       "...          ...         ...\n",
       "1984  -79.139649  2008-08-14\n",
       "1985  100.739258  2008-08-13\n",
       "1986  148.890625  2008-08-12\n",
       "1987  -52.030273  2008-08-11\n",
       "1988 -297.580078  2008-08-08\n",
       "\n",
       "[1989 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj = dj[dj.Open.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = []\n",
    "headlines = []\n",
    "\n",
    "#row now is an index series, where the series is of the form [Open, Date] pair, therefore call row[1] to only get the series, which is the value of each row\n",
    "for row in dj.iterrows():\n",
    "    daily_headlines = []\n",
    "    date = row[1][\"Date\"]\n",
    "    price.append(row[1]['Open'])\n",
    "    # collect news from the same date and put them into daily_headlines\n",
    "    for row_ in news[news.Date==date].iterrows():\n",
    "        daily_headlines.append(row_[1]['News'])\n",
    "    headlines.append(daily_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988\n",
      "1988\n"
     ]
    }
   ],
   "source": [
    "print(len(price))\n",
    "print(len(headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'0,0', '00', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'\\$', ' $ ', text)\n",
    "    text = re.sub(r'u s ', ' united states ', text)\n",
    "    text = re.sub(r'u n ', ' united nations ', text)\n",
    "    text = re.sub(r'u k ', ' united kingdom ', text)\n",
    "    text = re.sub(r'j k ', ' jk ', text)\n",
    "    text = re.sub(r' s ', ' ', text)\n",
    "    text = re.sub(r' yr ', ' year ', text)\n",
    "    text = re.sub(r' l g b t ', ' lgbt ', text)\n",
    "    text = re.sub(r'0km ', '0 km ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the headlines\n",
    "clean_headlines = []\n",
    "\n",
    "for daily_headlines in headlines:\n",
    "    clean_daily_headlines = []\n",
    "    for headline in daily_headlines:\n",
    "        clean_daily_headlines.append(clean_text(headline))\n",
    "    clean_headlines.append(clean_daily_headlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jamaica proposes marijuana dispensers tourists airports following legalisation kiosks desks would give people license purchase 2 ounces drug use stay',\n",
       " 'stephen hawking says pollution stupidity still biggest threats mankind certainly become less greedy less stupid treatment environment past decade',\n",
       " 'boris johnson says run tory party leadership',\n",
       " 'six gay men ivory coast abused forced flee homes pictured signing condolence book victims recent attack gay nightclub florida',\n",
       " 'switzerland denies citizenship muslim immigrant girls refused swim boys report',\n",
       " 'palestinian terrorist stabs israeli teen girl death bedroom',\n",
       " 'puerto rico default $ 1 billion debt friday',\n",
       " 'republic ireland fans awarded medal sportsmanship paris mayor',\n",
       " 'afghan suicide bomber kills 40 bbc news',\n",
       " 'us airstrikes kill least 250 isis fighters convoy outside fallujah official says',\n",
       " 'turkish cop took istanbul gunman hailed hero',\n",
       " 'cannabis compounds could treat alzheimer removing plaque forming proteins brain cells research suggests',\n",
       " 'japan top court approved blanket surveillance country muslims made us terrorist suspects never anything wrong says japanese muslim mohammed fujita',\n",
       " 'cia gave romania millions host secret prisons',\n",
       " 'groups urge united nations suspend saudi arabia rights council',\n",
       " 'googles free wifi indian railway stations better countrys paid services',\n",
       " 'mounting evidence suggests hobbits wiped modern humans ancestors 50000 years ago',\n",
       " 'men carried tuesday terror attack istanbul ataturk airport russia uzbekistan kyrgyzstan turkish offical said',\n",
       " 'calls suspend saudi arabia un human rights council military aggresion yemen',\n",
       " '100 nobel laureates call greenpeace anti gmo obstruction developing world',\n",
       " 'british pedophile sentenced 85 years us trafficking child abuse images domminich shaw kingpin sexual violence children sent dozens images online discussed plans assault kill child probation',\n",
       " 'us permitted 1 200 offshore fracks gulf mexico 2010 2014 allowed 72 billion gallons chemical discharge 2014',\n",
       " 'swimming ridicule french beach police carry guns swimming trunks police lifeguards frances busiest beaches carry guns bullet proof vests first time summer amid fears terrorists could target holidaymakers',\n",
       " 'uefa says minutes silence istanbul victims euro 2016 turkey already eliminated',\n",
       " 'law enforcement sources gun used paris terrorist attacks came phoenix']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_headlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 35190\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "\n",
    "for date in clean_headlines:\n",
    "    for headline in date:\n",
    "        for word in headline.split():\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 1\n",
    "            else:\n",
    "                word_counts[word] += 1\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embeddings: 2196016\n"
     ]
    }
   ],
   "source": [
    "# load GLoVe's embeddings\n",
    "embeddings_index = {}\n",
    "with open('C:/Users/edwar/Projects/Stock-Prediction/dataset/glove.840B.300d.txt', encoding ='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "    f.close()        \n",
    "print(\"word embeddings:\", len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from GloVe: 47\n",
      "Percent of words that are missing from vocabulary: 0.13%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from GloVe, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 10\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from GloVe:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Unique Words: 35190\n",
      "Number of Words we will use: 31265\n",
      "Percent of Words we will use: 88.85%\n"
     ]
    }
   ],
   "source": [
    "# Use only the words that appear more than a set threashold or that appears in GLoVe.\n",
    "# creates two dictionary that gives indexing value to each word that is used\n",
    " \n",
    "value = 0\n",
    "vocab_to_int = {}\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value \n",
    "        value += 1\n",
    "        \n",
    "# Special tokens that will be added to our vocab <UNK> = unknown, <PAD> = \n",
    "codes = [\"<UNK>\",\"<PAD>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total Number of Unique Words:\", len(word_counts))\n",
    "print(\"Number of Words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of Words we will use: {}%\".format(usage_ratio))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31263\n",
      "31264\n"
     ]
    }
   ],
   "source": [
    "for code in codes:\n",
    "    print(vocab_to_int[code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31265\n"
     ]
    }
   ],
   "source": [
    "#Create New embeddings for words in word_counts that are not in embedding_index \n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "#total number of words to use in training\n",
    "\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "#create a embedding matrix, each row is for a word, with total of 300 columns for the dimension of the GloVe vectors\n",
    "\n",
    "word_embedding_matrix = np.zeros((nb_words,embedding_dim))\n",
    "\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        #create a new embedding vectors with random values for words not in GLoVe embedding\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        # we also add this newly generated random embedding to the embedding_index along with the rest of the GloVe embedding\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "        \n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the words in the embedding \"including the pretrained ones\" will be updated as the model train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 615989\n",
      "Total number of UNKs in headlines: 5262\n",
      "Percent of words that are UNK: 0.8542%\n"
     ]
    }
   ],
   "source": [
    "# now to create a replicated headlines where all the words are transformed into their regarding index in vocab_to_int \n",
    "# int_headlines: all the headlines\n",
    "# int_daily_headlines: all the headlines in a day\n",
    "# int_headline: single headline in a day \n",
    "# int_headlines[int_daily_headlines[int_headline]]\n",
    "\n",
    "# for words in the headlines that are not in vocab_to_int \n",
    "# (meaning the word didn't hit the threshold and isn't in the pretrained embedding index)\n",
    "# we replace it with the <UNK> token for \"unknown\"\n",
    "word_count = 0\n",
    "unk_count = 0 \n",
    "\n",
    "int_headlines = []\n",
    "\n",
    "for date in clean_headlines:\n",
    "    int_daily_headlines = []\n",
    "    for headline in date:\n",
    "        int_headline = []\n",
    "        for word in headline.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                int_headline.append(vocab_to_int[word])\n",
    "            else:\n",
    "                int_headline.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        int_daily_headlines.append(int_headline)\n",
    "    int_headlines.append(int_daily_headlines)\n",
    "\n",
    "unk_percent = round((unk_count/word_count)*100,4)\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length of individual headline\n",
    "lengths = []\n",
    "for date in int_headlines:\n",
    "    for headline in date:\n",
    "        lengths.append(len(headline))\n",
    "\n",
    "lengths = pd.DataFrame(lengths, columns=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>49693.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.395891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.790246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "count  49693.000000\n",
       "mean      12.395891\n",
       "std        6.790246\n",
       "min        1.000000\n",
       "25%        7.000000\n",
       "50%       10.000000\n",
       "75%       16.000000\n",
       "max       41.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the length of total words per day to 200 words\n",
    "# and headlines to 16 words to reduce training time\n",
    "\n",
    "max_headline_length = 16\n",
    "max_daily_length = 200\n",
    "# pad_headlines only divides up the news up to date, not individual news (2D)\n",
    "# for news length that does not reach the max_daily_length, we pad it with the <PAD> token \n",
    "\n",
    "pad_headlines = []\n",
    "\n",
    "for date in int_headlines:\n",
    "    pad_daily_headlines = []\n",
    "    for headline in date:\n",
    "        #add the headline to pad daily headline if headline is less than max length\n",
    "        if len(headline) <= max_headline_length:\n",
    "            for word in headline:\n",
    "                #needs to append word by word so pad_daily_headline is 1D\n",
    "                pad_daily_headlines.append(word)\n",
    "        else:\n",
    "            headline = headline[:max_headline_length]\n",
    "            for word in headline:\n",
    "                pad_daily_headlines.append(word)\n",
    "    if len(pad_daily_headlines) < max_daily_length:\n",
    "        for i in range(max_daily_length - len(pad_daily_headlines)):\n",
    "            pad = vocab_to_int[\"<PAD>\"]\n",
    "            pad_daily_headlines.append(pad)\n",
    "    else:\n",
    "        pad_daily_headlines = pad_daily_headlines[:max_daily_length]\n",
    "    pad_headlines.append(pad_daily_headlines)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize (between [0,1]) the opening prices(target values, response variable)\n",
    "max_price = max(price)\n",
    "min_price = min(price)\n",
    "mean_price = np.mean(price)\n",
    "\n",
    "def normalize(price):\n",
    "    return ((price-min_price)/(max_price-min_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_price = []\n",
    "for p in price:\n",
    "    norm_price.append(normalize(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.5448422454901358\n"
     ]
    }
   ],
   "source": [
    "# checking normalization\n",
    "print(min(norm_price))\n",
    "print(max(norm_price))\n",
    "print(np.mean(norm_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train test split\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(pad_headlines, norm_price, test_size=0.15, random_state=2)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1689\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "# Check the lengths of train and test set\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_length1 = 3\n",
    "filter_length2 = 5\n",
    "dropout = 0.5\n",
    "learning_rate= 0.001\n",
    "weights = initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=2)\n",
    "nb_filter = 16\n",
    "rnn_output_size = 128\n",
    "hidden_dims = 128\n",
    "wider = True\n",
    "deeper = True\n",
    "\n",
    "if wider == True:\n",
    "    nb_filter *= 2\n",
    "    rnn_output_size *= 2\n",
    "    hidden_dims *= 2\n",
    "\n",
    "def build_model():\n",
    "    model1 = Sequential()\n",
    "    \n",
    "    model1.add(Embedding(nb_words,\n",
    "                        embedding_dim,\n",
    "                        weights=[word_embedding_matrix],\n",
    "                        input_length=max_daily_length))\n",
    "    model1.add(Dropout(dropout))\n",
    "    model1.add(Convolution1D(filters = nb_filter,\n",
    "                            kernel_size=filter_length1,\n",
    "                            padding = 'same',\n",
    "                            activation = 'relu'))\n",
    "    model1.add(Dropout(dropout))\n",
    "    if deeper == True:\n",
    "        model1.add(Convolution1D(filters = nb_filter,\n",
    "                                kernel_size = filter_length1,\n",
    "                                padding = 'same',\n",
    "                                activation = 'relu'))\n",
    "        model1.add(Dropout(dropout))\n",
    "    \n",
    "    model1.add(LSTM(rnn_output_size,\n",
    "                   activation = None,\n",
    "                   kernel_initializer = weights,\n",
    "                   dropout = dropout))\n",
    "    \n",
    "    ######\n",
    "    \n",
    "#     model2 = Sequential()\n",
    "    \n",
    "#     model2.add(Embedding(nb_words,\n",
    "#                         embedding_dim,\n",
    "#                         weights = [word_embedding_matrix],\n",
    "#                         input_length = max_daily_length))\n",
    "#     model2.add(Dropout(dropout))\n",
    "    \n",
    "#     model2.add(Convolution1D(filters = nb_filter,\n",
    "#                             kernel_size = filter_length2,\n",
    "#                             padding = 'same',\n",
    "#                             activation = 'relu'))\n",
    "    \n",
    "#     model2.add(Dropout(dropout))\n",
    "    \n",
    "#     if deeper == True:\n",
    "#         model2.add(Convolution1D(filters = nb_filter,\n",
    "#                                  kernel_size = filter_length2,\n",
    "#                                  padding = 'same',\n",
    "#                                  activation = 'relu'))\n",
    "#         model2.add(Dropout(dropout))\n",
    "        \n",
    "#     model2.add(LSTM(rnn_output_size,\n",
    "#                     activation=None,\n",
    "#                     kernel_initializer=weights,\n",
    "#                     dropout = dropout))\n",
    "    \n",
    "    #####\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(model1)\n",
    "    \n",
    "    model.add(Dense(hidden_dims, kernel_initializer=weights))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    if deeper == True:\n",
    "        model.add(Dense(hidden_dims//2, kernel_initializer=weights))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "    model.add(Dense(1,\n",
    "                   kernel_initializer=weights,\n",
    "                    name='output'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error',\n",
    "                 optimizer=Adam(lr=learning_rate,clipvalue=1.0))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current model: Deeper=False, Wider=True, LR=0.001, Dropout=0.3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edwar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1435 samples, validate on 254 samples\n",
      "Epoch 1/100\n",
      "1435/1435 [==============================] - 8s 6ms/step - loss: 0.2799 - val_loss: 0.0125\n",
      "Epoch 2/100\n",
      "1435/1435 [==============================] - 9s 6ms/step - loss: 0.0513 - val_loss: 0.0267\n",
      "Epoch 3/100\n",
      "1435/1435 [==============================] - 9s 7ms/step - loss: 0.0347 - val_loss: 0.0264\n",
      "Epoch 4/100\n",
      "1435/1435 [==============================] - 9s 7ms/step - loss: 0.0292 - val_loss: 0.0201\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 5/100\n",
      "1435/1435 [==============================] - 10s 7ms/step - loss: 0.0257 - val_loss: 0.0212\n",
      "Epoch 6/100\n",
      "1435/1435 [==============================] - 12s 8ms/step - loss: 0.0258 - val_loss: 0.0197\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Current model: Deeper=False, Wider=True, LR=0.001, Dropout=0.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edwar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1435 samples, validate on 254 samples\n",
      "Epoch 1/100\n",
      "1435/1435 [==============================] - 14s 10ms/step - loss: 0.4195 - val_loss: 0.0338\n",
      "Epoch 2/100\n",
      "1435/1435 [==============================] - 16s 11ms/step - loss: 0.1126 - val_loss: 0.0868\n",
      "Epoch 3/100\n",
      "1435/1435 [==============================] - 17s 12ms/step - loss: 0.0803 - val_loss: 0.0404\n",
      "Epoch 4/100\n",
      "1435/1435 [==============================] - 17s 12ms/step - loss: 0.0619 - val_loss: 0.0503\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 5/100\n",
      "1435/1435 [==============================] - 17s 12ms/step - loss: 0.0521 - val_loss: 0.0433\n",
      "Epoch 6/100\n",
      "1435/1435 [==============================] - 17s 12ms/step - loss: 0.0471 - val_loss: 0.0353\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Current model: Deeper=False, Wider=False, LR=0.001, Dropout=0.3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edwar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1435 samples, validate on 254 samples\n",
      "Epoch 1/100\n",
      "1435/1435 [==============================] - 17s 12ms/step - loss: 0.3036 - val_loss: 0.0156\n",
      "Epoch 2/100\n",
      "1435/1435 [==============================] - 18s 12ms/step - loss: 0.0615 - val_loss: 0.0350\n",
      "Epoch 3/100\n",
      "1435/1435 [==============================] - 18s 12ms/step - loss: 0.0372 - val_loss: 0.0280\n",
      "Epoch 4/100\n",
      "1435/1435 [==============================] - 18s 13ms/step - loss: 0.0281 - val_loss: 0.0190\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 5/100\n",
      "1435/1435 [==============================] - 18s 13ms/step - loss: 0.0269 - val_loss: 0.0192\n",
      "Epoch 6/100\n",
      "1435/1435 [==============================] - 18s 13ms/step - loss: 0.0223 - val_loss: 0.0214\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Current model: Deeper=False, Wider=False, LR=0.001, Dropout=0.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edwar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1435 samples, validate on 254 samples\n",
      "Epoch 1/100\n",
      "1435/1435 [==============================] - 18s 13ms/step - loss: 0.4513 - val_loss: 0.0240\n",
      "Epoch 2/100\n",
      "1435/1435 [==============================] - 18s 12ms/step - loss: 0.1228 - val_loss: 0.0782\n",
      "Epoch 3/100\n",
      "1435/1435 [==============================] - 19s 13ms/step - loss: 0.0784 - val_loss: 0.0494\n",
      "Epoch 4/100\n",
      "1435/1435 [==============================] - 18s 13ms/step - loss: 0.0595 - val_loss: 0.0423\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 5/100\n",
      "1435/1435 [==============================] - 19s 13ms/step - loss: 0.0526 - val_loss: 0.0407\n",
      "Epoch 6/100\n",
      "1435/1435 [==============================] - 19s 13ms/step - loss: 0.0492 - val_loss: 0.0402\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "for deeper in [False]:\n",
    "    for wider in [True,False]:\n",
    "        for learning_rate in [0.001]:\n",
    "            for dropout in [0.3, 0.5]:\n",
    "                model = build_model()\n",
    "                print()\n",
    "                print(\"Current model: Deeper={}, Wider={}, LR={}, Dropout={}\".format(\n",
    "                    deeper,wider,learning_rate,dropout))\n",
    "                print()\n",
    "                save_best_weights = 'question_pairs_weights_deeper={}_wider={}_lr={}_dropout={}.h5'.format(deeper,wider,learning_rate,dropout)\n",
    "                callbacks = [ModelCheckpoint(save_best_weights, monitor='val_loss', save_best_only=True),\n",
    "                            EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto'),\n",
    "                            ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=3)]\n",
    "                history= model.fit(x_train,\n",
    "                                  y_train,\n",
    "                                  batch_size=128,\n",
    "                                  epochs=100,\n",
    "                                  validation_split=0.15,\n",
    "                                  verbose=True,\n",
    "                                  shuffle=True,\n",
    "                                  callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 2s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the best weights\n",
    "deeper=False\n",
    "wider=True\n",
    "dropout=0.3\n",
    "learning_Rate=0.001\n",
    "\n",
    "model=build_model()\n",
    "\n",
    "model.load_weights('./question_pairs_weights_deeper={}_wider={}_lr={}_dropout={}.h5'.format(\n",
    "                    deeper,wider,learning_rate,dropout))\n",
    "predictions = model.predict(x_test, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012834476971068648"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(price):\n",
    "    price = price*(max_price-min_price)+min_price\n",
    "    return(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnorm_predictions=[]\n",
    "for pred in predictions:\n",
    "    unnorm_predictions.append(unnormalize(pred))\n",
    "    \n",
    "unnorm_y_test = []\n",
    "for y in y_test:\n",
    "    unnorm_y_test.append(unnormalize(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119.4104005781245"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the median absolute error for the predictions\n",
    "mae(unnorm_y_test, unnorm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of actual opening price changes\n",
      "                 \n",
      "count  299.000000\n",
      "mean    -7.094101\n",
      "std    139.532324\n",
      "min   -541.050782\n",
      "25%    -87.465332\n",
      "50%    -10.759766\n",
      "75%     54.689941\n",
      "max    673.139648\n",
      "\n",
      "Summary of predicted opening price changes\n",
      "                 \n",
      "count  299.000000\n",
      "mean    31.592124\n",
      "std    127.490018\n",
      "min   -284.840576\n",
      "25%    -52.418884\n",
      "50%     18.375427\n",
      "75%    113.680908\n",
      "max    576.110474\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary of actual opening price changes\")\n",
    "print(pd.DataFrame(unnorm_y_test, columns=[\"\"]).describe())\n",
    "print()\n",
    "print(\"Summary of predicted opening price changes\")\n",
    "print(pd.DataFrame(unnorm_predictions, columns=[\"\"]).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to measure if opening price increased or decreased\n",
    "direction_pred = []\n",
    "for pred in unnorm_predictions:\n",
    "    if pred >= 0:\n",
    "        direction_pred.append(1)\n",
    "    else:\n",
    "        direction_pred.append(0)\n",
    "direction_test = []\n",
    "for value in unnorm_y_test:\n",
    "    if value >= 0:\n",
    "        direction_test.append(1)\n",
    "    else:\n",
    "        direction_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values matched the actual direction 49.5% of the time.\n"
     ]
    }
   ],
   "source": [
    "# Calculate if the predicted direction matched the actual direction\n",
    "direction = acc(direction_test, direction_pred)\n",
    "direction = round(direction,4)*100\n",
    "print(\"Predicted values matched the actual direction {}% of the time.\".format(direction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
